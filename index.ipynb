{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll learn how to use both Adaboost and Gradient Boosting Classifiers from scikit-learn!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Compare and contrast Adaboost and Gradient Boosting\n",
    "* Use adaboost to make predictions on a dataset\n",
    "* Use Gradient Boosting to make predictions on a dataset\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this lab, we'll learn how to use Boosting algorithms to make classifications on the [Pima Indians Dataset](http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names). You will find the data stored within the file `pima-indians-diabetes.csv`. Our goal is to use boosting algorithms to classify each person as having or not having diabetes. Let's get started!\n",
    "\n",
    "We'll begin by importing everything we need for this lab. In the cell below:\n",
    "\n",
    "* Import `numpy`, `pandas`, and `matplotlib.pyplot`, and set the standard alias for each. Also set matplotlib visualizations to display inline. \n",
    "* Set a random seed of `0` by using `np.random.seed(0)`\n",
    "* Import `train_test_split` and `cross_val_score` from `sklearn.model_selection`\n",
    "* Import `StandardScaler` from `sklearn.preprocessing`\n",
    "* Import `AdaboostClassifier` and `GradientBoostingClassifier` from `sklearn.ensemble`\n",
    "* Import `accuracy_score`, `f1_score`, `confusion_matrix`, and `classification_report` from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use pandas to read in the data stored in `pima-indians-diabetes.csv` and store it in a DataFrame. Display the head to inspect the data we've imported and ensure everything loaded correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning, Exploration, and Preprocessing\n",
    "\n",
    "The target we're trying to predict is the `'Outcome'` column. A `1` denotes a patient with diabetes. \n",
    "\n",
    "By now, you're quite familiar with exploring and preprocessing a dataset, so we won't hold your hand for this step. \n",
    "\n",
    "In the following cells:\n",
    "\n",
    "* Store our target column in a separate variable and remove it from the dataset\n",
    "* Check for null values and deal with them as you see fit (if any exist)\n",
    "* Check the distribution of our target\n",
    "* Scale the dataset\n",
    "* Split the dataset into training and testing sets, with a `test_size` of `0.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Outcome']\n",
    "df = df.drop('Outcome',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.639947</td>\n",
       "      <td>0.848324</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>1.425995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-1.123396</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.530902</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>-0.365061</td>\n",
       "      <td>-0.190672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>1.943724</td>\n",
       "      <td>-0.263941</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-1.103255</td>\n",
       "      <td>0.604397</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.998208</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.123302</td>\n",
       "      <td>-0.494043</td>\n",
       "      <td>-0.920763</td>\n",
       "      <td>-1.041549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>0.504055</td>\n",
       "      <td>-1.504687</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>0.765836</td>\n",
       "      <td>1.409746</td>\n",
       "      <td>5.484909</td>\n",
       "      <td>-0.020496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.342981</td>\n",
       "      <td>-0.153185</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.811341</td>\n",
       "      <td>-0.818079</td>\n",
       "      <td>-0.275760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.250952</td>\n",
       "      <td>-1.342476</td>\n",
       "      <td>-0.987710</td>\n",
       "      <td>0.719086</td>\n",
       "      <td>0.071204</td>\n",
       "      <td>-0.125977</td>\n",
       "      <td>-0.676133</td>\n",
       "      <td>-0.616111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>-0.184482</td>\n",
       "      <td>-3.572597</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.419775</td>\n",
       "      <td>-1.020427</td>\n",
       "      <td>-0.360847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.547919</td>\n",
       "      <td>2.381884</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>1.534551</td>\n",
       "      <td>4.021922</td>\n",
       "      <td>-0.189437</td>\n",
       "      <td>-0.947944</td>\n",
       "      <td>1.681259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>0.128489</td>\n",
       "      <td>1.390387</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-4.060474</td>\n",
       "      <td>-0.724455</td>\n",
       "      <td>1.766346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.046014</td>\n",
       "      <td>-0.340968</td>\n",
       "      <td>1.183596</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.711690</td>\n",
       "      <td>-0.848280</td>\n",
       "      <td>-0.275760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>1.474267</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.762457</td>\n",
       "      <td>0.196681</td>\n",
       "      <td>0.064591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>0.566649</td>\n",
       "      <td>0.563223</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.620962</td>\n",
       "      <td>2.926869</td>\n",
       "      <td>2.021610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>2.131507</td>\n",
       "      <td>-0.470732</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>6.652839</td>\n",
       "      <td>-0.240205</td>\n",
       "      <td>-0.223115</td>\n",
       "      <td>2.191785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.342981</td>\n",
       "      <td>1.411672</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>-0.096379</td>\n",
       "      <td>0.826616</td>\n",
       "      <td>-0.785957</td>\n",
       "      <td>0.347687</td>\n",
       "      <td>1.511083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.936914</td>\n",
       "      <td>-0.653939</td>\n",
       "      <td>-3.572597</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.252897</td>\n",
       "      <td>0.036615</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>-0.090591</td>\n",
       "      <td>0.770014</td>\n",
       "      <td>1.660007</td>\n",
       "      <td>1.304175</td>\n",
       "      <td>1.752428</td>\n",
       "      <td>0.238963</td>\n",
       "      <td>-0.190672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.936914</td>\n",
       "      <td>-0.434859</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.303664</td>\n",
       "      <td>-0.658012</td>\n",
       "      <td>-0.190672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.560048</td>\n",
       "      <td>-2.021665</td>\n",
       "      <td>1.095454</td>\n",
       "      <td>0.027790</td>\n",
       "      <td>1.435129</td>\n",
       "      <td>-0.872441</td>\n",
       "      <td>-0.020496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.184482</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.593630</td>\n",
       "      <td>0.140667</td>\n",
       "      <td>0.330932</td>\n",
       "      <td>0.172520</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.250952</td>\n",
       "      <td>0.159787</td>\n",
       "      <td>0.976805</td>\n",
       "      <td>1.283638</td>\n",
       "      <td>1.347590</td>\n",
       "      <td>0.927452</td>\n",
       "      <td>0.701041</td>\n",
       "      <td>-0.531023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>-0.685236</td>\n",
       "      <td>0.770014</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.432467</td>\n",
       "      <td>-0.253316</td>\n",
       "      <td>1.425995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.936914</td>\n",
       "      <td>2.350587</td>\n",
       "      <td>1.080200</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.990912</td>\n",
       "      <td>-0.063049</td>\n",
       "      <td>0.660206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.530847</td>\n",
       "      <td>-0.059293</td>\n",
       "      <td>0.563223</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.379816</td>\n",
       "      <td>-0.630831</td>\n",
       "      <td>-0.360847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.124780</td>\n",
       "      <td>0.691838</td>\n",
       "      <td>1.286991</td>\n",
       "      <td>0.781814</td>\n",
       "      <td>0.574812</td>\n",
       "      <td>0.584771</td>\n",
       "      <td>-0.658012</td>\n",
       "      <td>1.511083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>0.128489</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.342717</td>\n",
       "      <td>0.305642</td>\n",
       "      <td>-0.113285</td>\n",
       "      <td>-0.805998</td>\n",
       "      <td>0.660206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.936914</td>\n",
       "      <td>0.817027</td>\n",
       "      <td>0.356432</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.940144</td>\n",
       "      <td>-0.648952</td>\n",
       "      <td>0.830381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.747831</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>-0.347291</td>\n",
       "      <td>0.522715</td>\n",
       "      <td>-1.115947</td>\n",
       "      <td>0.045675</td>\n",
       "      <td>-0.956462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.718712</td>\n",
       "      <td>0.754432</td>\n",
       "      <td>0.666618</td>\n",
       "      <td>-0.096379</td>\n",
       "      <td>0.262228</td>\n",
       "      <td>-1.242867</td>\n",
       "      <td>-0.685193</td>\n",
       "      <td>2.021610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.342981</td>\n",
       "      <td>-0.121888</td>\n",
       "      <td>1.183596</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.267472</td>\n",
       "      <td>-0.407342</td>\n",
       "      <td>0.404942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>-0.547919</td>\n",
       "      <td>-0.685236</td>\n",
       "      <td>-0.470732</td>\n",
       "      <td>-0.221835</td>\n",
       "      <td>0.696373</td>\n",
       "      <td>0.584771</td>\n",
       "      <td>-0.057009</td>\n",
       "      <td>-1.041549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.591345</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.952836</td>\n",
       "      <td>-0.540228</td>\n",
       "      <td>0.745293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>2.124780</td>\n",
       "      <td>-0.027996</td>\n",
       "      <td>0.563223</td>\n",
       "      <td>1.032726</td>\n",
       "      <td>0.609544</td>\n",
       "      <td>1.308210</td>\n",
       "      <td>0.945671</td>\n",
       "      <td>1.255820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>-0.250952</td>\n",
       "      <td>-0.591345</td>\n",
       "      <td>-1.297896</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>0.123302</td>\n",
       "      <td>-0.151361</td>\n",
       "      <td>-0.217075</td>\n",
       "      <td>-0.616111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.372265</td>\n",
       "      <td>-0.574128</td>\n",
       "      <td>-0.159107</td>\n",
       "      <td>0.314325</td>\n",
       "      <td>-0.443275</td>\n",
       "      <td>-0.763716</td>\n",
       "      <td>-0.956462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>1.530847</td>\n",
       "      <td>0.597947</td>\n",
       "      <td>1.286991</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.089785</td>\n",
       "      <td>0.791645</td>\n",
       "      <td>1.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>2.718712</td>\n",
       "      <td>1.004810</td>\n",
       "      <td>0.976805</td>\n",
       "      <td>1.032726</td>\n",
       "      <td>0.522715</td>\n",
       "      <td>1.092447</td>\n",
       "      <td>2.120497</td>\n",
       "      <td>0.490030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>2.421746</td>\n",
       "      <td>-0.653939</td>\n",
       "      <td>0.770014</td>\n",
       "      <td>0.781814</td>\n",
       "      <td>0.218813</td>\n",
       "      <td>-0.252897</td>\n",
       "      <td>0.048695</td>\n",
       "      <td>1.085644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.817027</td>\n",
       "      <td>1.286991</td>\n",
       "      <td>1.283638</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>2.196645</td>\n",
       "      <td>-0.343920</td>\n",
       "      <td>-0.531023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-1.248585</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>1.283638</td>\n",
       "      <td>-0.197966</td>\n",
       "      <td>1.815887</td>\n",
       "      <td>1.884928</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>-0.250952</td>\n",
       "      <td>2.068912</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.091805</td>\n",
       "      <td>1.043689</td>\n",
       "      <td>0.559387</td>\n",
       "      <td>-0.192914</td>\n",
       "      <td>0.234767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.639947</td>\n",
       "      <td>1.286484</td>\n",
       "      <td>-0.367337</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.976336</td>\n",
       "      <td>-0.887541</td>\n",
       "      <td>1.425995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.046014</td>\n",
       "      <td>0.472758</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.100593</td>\n",
       "      <td>2.144658</td>\n",
       "      <td>-0.956462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.459827</td>\n",
       "      <td>1.158182</td>\n",
       "      <td>-0.050356</td>\n",
       "      <td>0.889377</td>\n",
       "      <td>-0.636871</td>\n",
       "      <td>-0.445935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-0.250952</td>\n",
       "      <td>-0.403562</td>\n",
       "      <td>-0.367337</td>\n",
       "      <td>0.217261</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.760573</td>\n",
       "      <td>-0.751636</td>\n",
       "      <td>-0.701198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>1.881130</td>\n",
       "      <td>0.976805</td>\n",
       "      <td>1.471822</td>\n",
       "      <td>3.735386</td>\n",
       "      <td>1.435129</td>\n",
       "      <td>-0.754656</td>\n",
       "      <td>-0.616111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>1.036107</td>\n",
       "      <td>0.459827</td>\n",
       "      <td>0.719086</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.051710</td>\n",
       "      <td>-0.087210</td>\n",
       "      <td>1.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.222381</td>\n",
       "      <td>0.976805</td>\n",
       "      <td>1.158182</td>\n",
       "      <td>0.262228</td>\n",
       "      <td>0.572079</td>\n",
       "      <td>1.767143</td>\n",
       "      <td>0.319855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.936914</td>\n",
       "      <td>0.504055</td>\n",
       "      <td>1.080200</td>\n",
       "      <td>1.283638</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>-0.244256</td>\n",
       "      <td>0.490030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>0.065895</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.546695</td>\n",
       "      <td>-0.645932</td>\n",
       "      <td>1.596171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.466156</td>\n",
       "      <td>0.356432</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.698998</td>\n",
       "      <td>-0.830159</td>\n",
       "      <td>-0.616111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.639947</td>\n",
       "      <td>2.162804</td>\n",
       "      <td>1.183596</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.445159</td>\n",
       "      <td>-0.585529</td>\n",
       "      <td>2.787399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>-0.547919</td>\n",
       "      <td>-1.029505</td>\n",
       "      <td>-0.574128</td>\n",
       "      <td>0.342717</td>\n",
       "      <td>-0.553964</td>\n",
       "      <td>-0.455967</td>\n",
       "      <td>0.888288</td>\n",
       "      <td>-0.956462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>1.530847</td>\n",
       "      <td>1.536861</td>\n",
       "      <td>0.253036</td>\n",
       "      <td>0.656358</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>1.523973</td>\n",
       "      <td>-0.208015</td>\n",
       "      <td>0.830381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>1.530847</td>\n",
       "      <td>-0.998208</td>\n",
       "      <td>-0.367337</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-1.204791</td>\n",
       "      <td>-0.996266</td>\n",
       "      <td>-0.020496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>-0.622642</td>\n",
       "      <td>0.356432</td>\n",
       "      <td>1.722735</td>\n",
       "      <td>0.870031</td>\n",
       "      <td>0.115169</td>\n",
       "      <td>-0.908682</td>\n",
       "      <td>2.532136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.547919</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.405445</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.610154</td>\n",
       "      <td>-0.398282</td>\n",
       "      <td>-0.531023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.342981</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.279594</td>\n",
       "      <td>-0.735190</td>\n",
       "      <td>-0.685193</td>\n",
       "      <td>-0.275760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.159787</td>\n",
       "      <td>-0.470732</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.240205</td>\n",
       "      <td>-0.371101</td>\n",
       "      <td>1.170732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.873019</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.656358</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.202129</td>\n",
       "      <td>-0.473785</td>\n",
       "      <td>-0.871374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.639947  0.848324  0.149641  0.907270 -0.692891  0.204013  0.468492   \n",
       "1   -0.844885 -1.123396 -0.160546  0.530902 -0.692891 -0.684422 -0.365061   \n",
       "2    1.233880  1.943724 -0.263941 -1.288212 -0.692891 -1.103255  0.604397   \n",
       "3   -0.844885 -0.998208 -0.160546  0.154533  0.123302 -0.494043 -0.920763   \n",
       "4   -1.141852  0.504055 -1.504687  0.907270  0.765836  1.409746  5.484909   \n",
       "5    0.342981 -0.153185  0.253036 -1.288212 -0.692891 -0.811341 -0.818079   \n",
       "6   -0.250952 -1.342476 -0.987710  0.719086  0.071204 -0.125977 -0.676133   \n",
       "7    1.827813 -0.184482 -3.572597 -1.288212 -0.692891  0.419775 -1.020427   \n",
       "8   -0.547919  2.381884  0.046245  1.534551  4.021922 -0.189437 -0.947944   \n",
       "9    1.233880  0.128489  1.390387 -1.288212 -0.692891 -4.060474 -0.724455   \n",
       "10   0.046014 -0.340968  1.183596 -1.288212 -0.692891  0.711690 -0.848280   \n",
       "11   1.827813  1.474267  0.253036 -1.288212 -0.692891  0.762457  0.196681   \n",
       "12   1.827813  0.566649  0.563223 -1.288212 -0.692891 -0.620962  2.926869   \n",
       "13  -0.844885  2.131507 -0.470732  0.154533  6.652839 -0.240205 -0.223115   \n",
       "14   0.342981  1.411672  0.149641 -0.096379  0.826616 -0.785957  0.347687   \n",
       "15   0.936914 -0.653939 -3.572597 -1.288212 -0.692891 -0.252897  0.036615   \n",
       "16  -1.141852 -0.090591  0.770014  1.660007  1.304175  1.752428  0.238963   \n",
       "17   0.936914 -0.434859  0.253036 -1.288212 -0.692891 -0.303664 -0.658012   \n",
       "18  -0.844885 -0.560048 -2.021665  1.095454  0.027790  1.435129 -0.872441   \n",
       "19  -0.844885 -0.184482  0.046245  0.593630  0.140667  0.330932  0.172520   \n",
       "20  -0.250952  0.159787  0.976805  1.283638  1.347590  0.927452  0.701041   \n",
       "21   1.233880 -0.685236  0.770014 -1.288212 -0.692891  0.432467 -0.253316   \n",
       "22   0.936914  2.350587  1.080200 -1.288212 -0.692891  0.990912 -0.063049   \n",
       "23   1.530847 -0.059293  0.563223  0.907270 -0.692891 -0.379816 -0.630831   \n",
       "24   2.124780  0.691838  1.286991  0.781814  0.574812  0.584771 -0.658012   \n",
       "25   1.827813  0.128489  0.046245  0.342717  0.305642 -0.113285 -0.805998   \n",
       "26   0.936914  0.817027  0.356432 -1.288212 -0.692891  0.940144 -0.648952   \n",
       "27  -0.844885 -0.747831 -0.160546 -0.347291  0.522715 -1.115947  0.045675   \n",
       "28   2.718712  0.754432  0.666618 -0.096379  0.262228 -1.242867 -0.685193   \n",
       "29   0.342981 -0.121888  1.183596 -1.288212 -0.692891  0.267472 -0.407342   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "738 -0.547919 -0.685236 -0.470732 -0.221835  0.696373  0.584771 -0.057009   \n",
       "739 -0.844885 -0.591345  0.253036 -1.288212 -0.692891  0.952836 -0.540228   \n",
       "740  2.124780 -0.027996  0.563223  1.032726  0.609544  1.308210  0.945671   \n",
       "741 -0.250952 -0.591345 -1.297896 -0.033651  0.123302 -0.151361 -0.217075   \n",
       "742 -0.844885 -0.372265 -0.574128 -0.159107  0.314325 -0.443275 -0.763716   \n",
       "743  1.530847  0.597947  1.286991 -1.288212 -0.692891  0.089785  0.791645   \n",
       "744  2.718712  1.004810  0.976805  1.032726  0.522715  1.092447  2.120497   \n",
       "745  2.421746 -0.653939  0.770014  0.781814  0.218813 -0.252897  0.048695   \n",
       "746 -0.844885  0.817027  1.286991  1.283638 -0.692891  2.196645 -0.343920   \n",
       "747 -0.844885 -1.248585  0.253036  1.283638 -0.197966  1.815887  1.884928   \n",
       "748 -0.250952  2.068912  0.046245  0.091805  1.043689  0.559387 -0.192914   \n",
       "749  0.639947  1.286484 -0.367337 -1.288212 -0.692891 -0.976336 -0.887541   \n",
       "750  0.046014  0.472758  0.046245 -1.288212 -0.692891 -0.100593  2.144658   \n",
       "751 -0.844885  0.003301  0.459827  1.158182 -0.050356  0.889377 -0.636871   \n",
       "752 -0.250952 -0.403562 -0.367337  0.217261 -0.692891 -0.760573 -0.751636   \n",
       "753 -1.141852  1.881130  0.976805  1.471822  3.735386  1.435129 -0.754656   \n",
       "754  1.233880  1.036107  0.459827  0.719086 -0.692891  0.051710 -0.087210   \n",
       "755 -0.844885  0.222381  0.976805  1.158182  0.262228  0.572079  1.767143   \n",
       "756  0.936914  0.504055  1.080200  1.283638 -0.692891  0.000942 -0.244256   \n",
       "757 -1.141852  0.065895  0.149641 -1.288212 -0.692891  0.546695 -0.645932   \n",
       "758 -0.844885 -0.466156  0.356432 -1.288212 -0.692891  0.698998 -0.830159   \n",
       "759  0.639947  2.162804  1.183596 -1.288212 -0.692891  0.445159 -0.585529   \n",
       "760 -0.547919 -1.029505 -0.574128  0.342717 -0.553964 -0.455967  0.888288   \n",
       "761  1.530847  1.536861  0.253036  0.656358 -0.692891  1.523973 -0.208015   \n",
       "762  1.530847 -0.998208 -0.367337 -1.288212 -0.692891 -1.204791 -0.996266   \n",
       "763  1.827813 -0.622642  0.356432  1.722735  0.870031  0.115169 -0.908682   \n",
       "764 -0.547919  0.034598  0.046245  0.405445 -0.692891  0.610154 -0.398282   \n",
       "765  0.342981  0.003301  0.149641  0.154533  0.279594 -0.735190 -0.685193   \n",
       "766 -0.844885  0.159787 -0.470732 -1.288212 -0.692891 -0.240205 -0.371101   \n",
       "767 -0.844885 -0.873019  0.046245  0.656358 -0.692891 -0.202129 -0.473785   \n",
       "\n",
       "            7  \n",
       "0    1.425995  \n",
       "1   -0.190672  \n",
       "2   -0.105584  \n",
       "3   -1.041549  \n",
       "4   -0.020496  \n",
       "5   -0.275760  \n",
       "6   -0.616111  \n",
       "7   -0.360847  \n",
       "8    1.681259  \n",
       "9    1.766346  \n",
       "10  -0.275760  \n",
       "11   0.064591  \n",
       "12   2.021610  \n",
       "13   2.191785  \n",
       "14   1.511083  \n",
       "15  -0.105584  \n",
       "16  -0.190672  \n",
       "17  -0.190672  \n",
       "18  -0.020496  \n",
       "19  -0.105584  \n",
       "20  -0.531023  \n",
       "21   1.425995  \n",
       "22   0.660206  \n",
       "23  -0.360847  \n",
       "24   1.511083  \n",
       "25   0.660206  \n",
       "26   0.830381  \n",
       "27  -0.956462  \n",
       "28   2.021610  \n",
       "29   0.404942  \n",
       "..        ...  \n",
       "738 -1.041549  \n",
       "739  0.745293  \n",
       "740  1.255820  \n",
       "741 -0.616111  \n",
       "742 -0.956462  \n",
       "743  1.000557  \n",
       "744  0.490030  \n",
       "745  1.085644  \n",
       "746 -0.531023  \n",
       "747 -0.105584  \n",
       "748  0.234767  \n",
       "749  1.425995  \n",
       "750 -0.956462  \n",
       "751 -0.445935  \n",
       "752 -0.701198  \n",
       "753 -0.616111  \n",
       "754  1.000557  \n",
       "755  0.319855  \n",
       "756  0.490030  \n",
       "757  1.596171  \n",
       "758 -0.616111  \n",
       "759  2.787399  \n",
       "760 -0.956462  \n",
       "761  0.830381  \n",
       "762 -0.020496  \n",
       "763  2.532136  \n",
       "764 -0.531023  \n",
       "765 -0.275760  \n",
       "766  1.170732  \n",
       "767 -0.871374  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler() \n",
    "scaled_df = scaler.fit_transform(X)\n",
    "scaled_df\n",
    "pd.DataFrame(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "\n",
    "Now that we've cleaned and preprocessed our dataset, we're ready to fit some models!\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create an `AdaBoostClassifier`\n",
    "* Create a `GradientBoostingClassifer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf = None\n",
    "gbt_clf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train each of the classifiers using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some predictions using each model so that we can calculate the training and testing accuracy for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds = None\n",
    "adaboost_test_preds = None\n",
    "gbt_clf_train_preds = None\n",
    "gbt_clf_test_preds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the following function and use it to calculate the training and testing accuracy and f1-score for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_acc_and_f1_score(true, preds, model_name):\n",
    "    acc = None\n",
    "    f1 = None\n",
    "    print(\"Model: {}\".format(None))\n",
    "    print(\"Accuracy: {}\".format(None))\n",
    "    print(\"F1-Score: {}\".format(None))\n",
    "    \n",
    "print(\"Training Metrics\")\n",
    "display_acc_and_f1_score(y_train, adaboost_train_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_train, gbt_clf_train_preds, model_name='Gradient Boosted Trees')\n",
    "print(\"\")\n",
    "print(\"Testing Metrics\")\n",
    "display_acc_and_f1_score(y_test, adaboost_test_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_test, gbt_clf_test_preds, model_name='Gradient Boosted Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go one step further and create a confusion matrix and classification report for each. Do so in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_confusion_matrix = None\n",
    "adaboost_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_confusion_matrix = None\n",
    "gbt_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_classification_report = None\n",
    "print(adaboost_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_classification_report = None\n",
    "print(gbt_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question:_** How did the models perform? Interpret the evaluation metrics above to answer this question.\n",
    "\n",
    "Write your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    " \n",
    " \n",
    "As a final performance check, let's calculate the `cross_val_score` for each model! Do so now in the cells below. \n",
    "\n",
    "Recall that to compute the cross validation score, we need to pass in:\n",
    "\n",
    "* a classifier\n",
    "* All training Data\n",
    "* All labels\n",
    "* The number of folds we want in our cross validation score. \n",
    "\n",
    "Since we're computing cross validation score, we'll want to pass in the entire (scaled) dataset, as well as all of the labels. We don't need to give it data that has been split into training and testing sets because it will handle this step during the cross validation. \n",
    "\n",
    "In the cells below, compute the mean cross validation score for each model. For the data, use our `scaled_df` variable. The corresponding labels are in the variable `target`. Also set `cv=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Adaboost Cross-Val Score (k=5):')\n",
    "print(None)\n",
    "# Expected Output: 0.7631270690094218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean GBT Cross-Val Score (k=5):')\n",
    "print(None)\n",
    "# Expected Output: 0.7591715474068416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models didn't do poorly, but we could probably do a bit better by tuning some of the important parameters such as the **_Learning Rate_**. \n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we learned how to use scikit-learn's implementations of popular boosting algorithms such as AdaBoost and Gradient Boosted Trees to make classification predictions on a real-world dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
